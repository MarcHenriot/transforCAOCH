{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "film_title = \"Transformers 2\""
      ],
      "metadata": {
        "id": "LKgDnYY8c2sG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tjRdl4TboWe",
        "outputId": "d85a3947-013c-4cf0-cdfb-0d84fbfe9a8e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.19.1-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 30.0 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 10.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.6.0-py3-none-any.whl (84 kB)\n",
            "\u001b[K     |████████████████████████████████| 84 kB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.6.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SzeBSA_UlsSd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import gdown\n",
        "from transformers import GPT2LMHeadModel,  GPT2Tokenizer, GPT2Config, GPT2LMHeadModel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://drive.google.com/drive/folders/1GdEtbxBPpsilJb21ZY6aP2oM1lkARgzu?usp=sharing\"\n",
        "gdown.download_folder(url, quiet=True, use_cookies=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLtYZ8nW6l0a",
        "outputId": "49cd815b-a465-42b0-fffb-1de9ba994d80"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/model_save/added_tokens.json',\n",
              " '/content/model_save/config.json',\n",
              " '/content/model_save/merges.txt',\n",
              " '/content/model_save/pytorch_model.bin',\n",
              " '/content/model_save/special_tokens_map.json',\n",
              " '/content/model_save/tokenizer_config.json',\n",
              " '/content/model_save/vocab.json']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"model_save\"\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "c-d9NhNLmSZn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(path, bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>', local_files_only=True)\n",
        "configuration = GPT2Config.from_pretrained(path, output_hidden_states=False, local_files_only=True)\n",
        "model = GPT2LMHeadModel.from_pretrained(path, config=configuration, local_files_only=True)\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncf84H2Kl9Z4",
        "outputId": "c761b9d8-c4c6-463a-9ff1-08b0374c27b2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(50259, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "model.eval()\n",
        "prompt = \"<|startoftext|>\"\n",
        "\n",
        "generated = torch.tensor(tokenizer.encode(prompt + film_title)).unsqueeze(0)\n",
        "generated = generated.to(device)\n",
        "\n",
        "print(generated)\n",
        "\n",
        "sample_outputs = model.generate(\n",
        "    generated, \n",
        "    do_sample=True,   \n",
        "    top_k=50, \n",
        "    max_length=400,\n",
        "    top_p=0.95, \n",
        "    num_return_sequences=3\n",
        ")\n",
        "\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "    print(f\"{i}: {tokenizer.decode(sample_output, skip_special_tokens=True)}\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtuOJFOCl3wf",
        "outputId": "4d606cb1-0108-4252-e6dd-b35ee9bd2ec9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[50257, 41762,   364,   362]], device='cuda:0')\n",
            "0: Transformers 2 : Ah ouais!!! Ça c’est du film, je suis très client de ce genre d’aventure. Ça c’est du lourd. Il y a certains pointe d’humour qui fond les mouvements de jeu vidéo mais ça assure le spectacle et son petit claque. Vraiment, ce n’est pas original de voir ces personnages qui sont dans leur mésaventures. C’est peut être l’homme qui voit sa femme et le jeu d’acteur du jeu, la femme, sa femme a un accident dans la femme ou les prostitués… mais c’est dans l’intrigue avec le jeu vidéo mais c’est surtout l’humour qui veut ça (malgré que ça doit être pas lourd mais pas lourdingue) : mais a l’histoire, je ne sais pas si c’est du jamais vu mais le rythme est bon. Déjà, on ne voit pas ce que on apprécie, je n’aurais pas voulu faire mieux que de l’aventure mais franchement : scénario bonne mais l’humour est bien ficelé et tiens bien l’aventure. Après, j’ai bien aimée l’aventure très captivant. Et c’est grâce aux personnages et bien surtout aux acteurs\n",
            "\n",
            "\n",
            "1: Transformers 2 : Alors la, je connais beaucoup de jeu vidéo et je revais pas forcement aux jeunes juges. Je fais mes critiques toujours autant celui la et ça envoie du lourd. Non vraiment, on rigole quand même à la fin, sa bouge tous et je pense que cette suite est un plus sérieux que seul. Un très bon trois fois de la trilogie, plus dure et plus mature (ce qui faut le dire donc plus mature et préféré, c’est que du bon). Et dans le deuxième partie, il y a une sacrée personnalité et une très bonne humeur, surtout le robot qui est à une humeur touchant. Il y a pas que les trois grands personnages principaux, un peu comme les trois frères qui on bien vieillie (c’est une histoire qui se voit être a la hauteur du rien et un jour, c’est un truck de ouf ou pas…). Et pour finir, comme je disais, le personnage de l’agent secret est plus convaincant. Car, il a beaucoup de rebondissement, il a pu de vivre et quand l’humour bien fait, ça dépasse mais il fallait quand même pas délirant, il y a de quoi rebondissement et de s’ennuyer sur une bonne humeur et un contrôle découvrir les robots mais bon, globalement c’est\n",
            "\n",
            "\n",
            "2: Transformers 2 : C’est encore une fois très beau, très bien construit et surtout très bien construit. Cette suite est vraiment très divertissent et remplit de bonne idée et de moyen. Avec une réalisation et une mise en scène efficace et des effets spéciaux très réussit. Vraiment ce premier épisode de la trilogie est une pure merveille, c’est une pur merveille. Remplit de bon effets spéciaux, de bonne idée, vraiment la force du premier : des fois on voit qu’il est devenu et pourquoi sa propre car il est magistralement bien mis en avant la suite. On nous présente avec plaisir notre maitre de la trilogie, c’est des maitres du vrai cinéma : l’efficace qui renforce l’émotion, l’émotion et moi je dis : bravo. Le scénario du film est très sympa, très original avec un coté très proche avec des gadgets qui fait la force de l’humour et de l’action bien dosé mais un final des plus explosifs et puissant mais au moins, c’est déjà énorme car on s’en fou. Ensuite, il faut dire que les acteurs sont très convaincant et crédibles. Andrew Garfield est vraiment très charismatique, l’unique et très amusant\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8OS5i56y6723"
      },
      "execution_count": 7,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "generate_txt.ipynb",
      "provenance": []
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}