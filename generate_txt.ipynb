{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "njRN-WDVmH6f",
        "outputId": "bdba975f-a5db-4cd7-db96-bc27b97ea297",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.19.1-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 34.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 46.1 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.6.0-py3-none-any.whl (84 kB)\n",
            "\u001b[K     |████████████████████████████████| 84 kB 3.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.6.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SzeBSA_UlsSd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from google.colab import drive\n",
        "from transformers import GPT2LMHeadModel,  GPT2Tokenizer, GPT2Config, GPT2LMHeadModel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "zLtYZ8nW6l0a",
        "outputId": "b08d6f5a-22b4-4df9-8453-b4a19ae698b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/DataIA/fandecaoch/model_save\"\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "c-d9NhNLmSZn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(path, bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>', local_files_only=True)\n",
        "configuration = GPT2Config.from_pretrained(path, output_hidden_states=False, local_files_only=True)\n",
        "model = GPT2LMHeadModel.from_pretrained(path, config=configuration, local_files_only=True)\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "id": "ncf84H2Kl9Z4",
        "outputId": "b3dc4e5e-f4dc-4b5c-b69d-0523a7a01c19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(50259, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "model.eval()\n",
        "prompt = \"<|startoftext|>\"\n",
        "film_title = \"Transformer 2\"\n",
        "\n",
        "generated = torch.tensor(tokenizer.encode(prompt+film_title)).unsqueeze(0)\n",
        "generated = generated.to(device)\n",
        "\n",
        "print(generated)\n",
        "\n",
        "sample_outputs = model.generate(\n",
        "    generated, \n",
        "    do_sample=True,   \n",
        "    top_k=50, \n",
        "    max_length=400,\n",
        "    top_p=0.95, \n",
        "    num_return_sequences=3\n",
        ")\n",
        "\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "  print(\"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "id": "mtuOJFOCl3wf",
        "outputId": "7853c142-0bf7-478c-f9c1-e953988f387f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[50257,  8291, 16354,   362]], device='cuda:0')\n",
            "0: Transformer 2 : C’est assez différent dans les anciens épisodes de la trilogie. Je l’ai vraiment aimée au cinéma avec cequel tout simplement culte. Quel plaisir de retrouvé les nouveaux personnages hauts en couleurs du feu de dieu. Mais, ça aurait pu faire de gags en gags mais ça a le mieux et de personnage quelque chose car il y a pas de dialogue pour les nouveaux : comme la femme le fait sur les dieux. Enfin, tous ça sert encore une fois rythmé, ça s’arrête pas mal dans l’action mais en plus, on y croit pas mal et le combat final est au rendez vous. Après, ils n’auraient pas trouvé une suite mais le premier n’est pas facile et tout le film s’emboit et l’ensemble reste efficace. Sinon, niveau scénario, il a du mal a lui reproche. C’est très cliché de notre société du cinéma mais il est bien dosé entre autres personnages. Et donc, ce que j’ai dit aimée, c’est que il y a quelque facilité qui se met en place mais ça va encore. Sinon, les acteurs sont très bons, j’adore toujours Christian Bale qui est écouté parfaitement dans son rôle de manipulateur.\n",
            "\n",
            "\n",
            "1: Transformer 2 : Une suite encore plus efficace que le premier. On retrouve un David Fincher (professeur de prisonnière, justice, évolution, mafieux…) qui nous livre une éventuel suite sans ennuie a la hauteur du premier (c'est un truck de ouf). Et bien dans cet épisode, on a le droit a une mémorable suite, qui est plus précurseur avec un rythmé qui est plus d’action mais tout est divertissent. Surtout quand on voit un peu a la Die Hard dans les années 80, car on voit que il nous découvre une nouvelle invention du virus qui sait être a peu plus étrange que des terroristes, ça ce voit vite. Mais on se rend compte pourquoi ce dernier épisode est les Die Hard préféré même si l’histoire est bien ficelée, bien ficelée avec beaucoup d’action et d’humour (je vous le conseil). Non vraiment, ce qui a de bien dans ce film, c’est que les personnages sont tellement plaisant et sympa à suivre dans leur délire, ce qui rend le personnage avec eux aussi plaisant (et le père fait pas vite, son père n’est pas sans critiqué avec sa fille, sa fille a toujours sa mère…). Et puis, le scénario est toujours aussi bien ficel\n",
            "\n",
            "\n",
            "2: Transformer 2 : Une suite qui mélange avec brio l’action et les personnages que je regarde. Et c’est grâce en partie a un scénario tout simplement original. Vraiment, il faut dire qu’a cette excellente idée de départ soit fidèle au bon boulot du premier opus. Car, c’est dans la simplicité que je n’avais pas vu un film avec une ambiance de malade : vraiment très bien maitrisé. Et c’est bien dommage que les trois premier épisodes sont bien mais la, ce n’est pas plus mal. Car, tout le film possède un rythme effréné, avec des scènes d’action vraiment sympa, il y a rien d’exceptionnelle, ou on dirait que ça serait quand même de l’action et ça serait bizarre avec les scènes d’actions. Et la, il y a un coté épique pour faire un géant, et l’humour est très sympa à suivre. Et tout comme l’action : c’est bien rythmé et efficace, plus on avance dans les trois premier épisodes. Car, l’histoire est vraiment bien ficelée : un homme (Michaël Youn) qui se fait kidnappé par le méchante (Franck Dubosc) qui est très appréciable et qui se permet de être kidnappé.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8OS5i56y6723"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "generate_txt.ipynb",
      "provenance": []
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}