{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hk3pz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import GPT2LMHeadModel,  GPT2Tokenizer, GPT2Config, GPT2LMHeadModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'data/data.csv'\n",
    "batch_size = 1\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "seed_val = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47326</td>\n",
       "      <td>La Passion du Christ : J’ai vraiment bien aimé...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30784</td>\n",
       "      <td>Predator 2 : Après le chef d’œuvre qui est le ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>142987</td>\n",
       "      <td>Predators : Après un premier épisode qui est u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43225</td>\n",
       "      <td>Predator : Un classique du film d’action fanta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>61545</td>\n",
       "      <td>Prête-moi ta main : Une comédie romantique plu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                            comment\n",
       "0   47326  La Passion du Christ : J’ai vraiment bien aimé...\n",
       "1   30784  Predator 2 : Après le chef d’œuvre qui est le ...\n",
       "2  142987  Predators : Après un premier épisode qui est u...\n",
       "3   43225  Predator : Un classique du film d’action fanta...\n",
       "4   61545  Prête-moi ta main : Une comédie romantique plu..."
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(train_path)  \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    La Passion du Christ : J’ai vraiment bien aimé...\n",
       "1    Predator 2 : Après le chef d’œuvre qui est le ...\n",
       "2    Predators : Après un premier épisode qui est u...\n",
       "3    Predator : Un classique du film d’action fanta...\n",
       "4    Prête-moi ta main : Une comédie romantique plu...\n",
       "Name: comment, dtype: object"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(inplace=True)\n",
    "comments = df.comment.copy()\n",
    "comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x207939e20d0>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "comment_lengths = []\n",
    "for comment in comments:\n",
    "    tokens = nltk.word_tokenize(comment)\n",
    "    comment_lengths.append(len(tokens))\n",
    "comment_lengths = np.array(comment_lengths)\n",
    "sns.displot(comment_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "372.0129600829445"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(comment_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max model length is 1024 for this model, although the actual embedding size for GPT small is 768\n",
      "The beginning of sequence token <|startoftext|> token has the id 50257\n",
      "The end of sequence token <|endoftext|> has the id 50256\n",
      "The padding token <|pad|> has the id 50258\n"
     ]
    }
   ],
   "source": [
    "print(\"The max model length is {} for this model, although the actual embedding size for GPT small is 768\".format(tokenizer.model_max_length))\n",
    "print(\"The beginning of sequence token {} token has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.bos_token_id), tokenizer.bos_token_id))\n",
    "print(\"The end of sequence token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.eos_token_id), tokenizer.eos_token_id))\n",
    "print(\"The padding token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.pad_token_id), tokenizer.pad_token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, txt_list, tokenizer, max_length):\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "\n",
    "        for txt in txt_list:\n",
    "            encodings_dict = tokenizer('<|startoftext|>'+ txt + '<|endoftext|>', truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
    "            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.attn_masks[idx] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1736 training samples\n",
      "193 validation samples\n"
     ]
    }
   ],
   "source": [
    "dataset = GPT2Dataset(comments, tokenizer, max_length=768)\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print(f'{train_size} training samples')\n",
    "print(f'{val_size} validation samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler = RandomSampler(train_dataset),\n",
    "    batch_size = batch_size\n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    sampler = SequentialSampler(val_dataset),\n",
    "    batch_size = batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "learning_rate = 5e-4\n",
    "warmup_steps = 1e2\n",
    "epsilon = 1e-8\n",
    "sample_every = 100\n",
    "total_steps = len(train_dataloader) * epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 4.00 GiB total capacity; 3.09 GiB already allocated; 0 bytes free; 3.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hk3pz\\Code\\fandecaoch\\train.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hk3pz/Code/fandecaoch/train.ipynb#ch0000014?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m GPT2LMHeadModel\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mgpt2\u001b[39m\u001b[39m\"\u001b[39m, config\u001b[39m=\u001b[39mconfiguration)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hk3pz/Code/fandecaoch/train.ipynb#ch0000014?line=2'>3</a>\u001b[0m model\u001b[39m.\u001b[39mresize_token_embeddings(\u001b[39mlen\u001b[39m(tokenizer))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/hk3pz/Code/fandecaoch/train.ipynb#ch0000014?line=4'>5</a>\u001b[0m model\u001b[39m.\u001b[39;49mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hk3pz/Code/fandecaoch/train.ipynb#ch0000014?line=6'>7</a>\u001b[0m optimizer \u001b[39m=\u001b[39m AdamW(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hk3pz/Code/fandecaoch/train.ipynb#ch0000014?line=7'>8</a>\u001b[0m     model\u001b[39m.\u001b[39mparameters(),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hk3pz/Code/fandecaoch/train.ipynb#ch0000014?line=8'>9</a>\u001b[0m     lr \u001b[39m=\u001b[39m learning_rate,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hk3pz/Code/fandecaoch/train.ipynb#ch0000014?line=9'>10</a>\u001b[0m     eps \u001b[39m=\u001b[39m epsilon\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hk3pz/Code/fandecaoch/train.ipynb#ch0000014?line=10'>11</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hk3pz/Code/fandecaoch/train.ipynb#ch0000014?line=12'>13</a>\u001b[0m scheduler \u001b[39m=\u001b[39m get_linear_schedule_with_warmup(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hk3pz/Code/fandecaoch/train.ipynb#ch0000014?line=13'>14</a>\u001b[0m     optimizer, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hk3pz/Code/fandecaoch/train.ipynb#ch0000014?line=14'>15</a>\u001b[0m     num_warmup_steps \u001b[39m=\u001b[39m warmup_steps, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hk3pz/Code/fandecaoch/train.ipynb#ch0000014?line=15'>16</a>\u001b[0m     num_training_steps \u001b[39m=\u001b[39m total_steps\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hk3pz/Code/fandecaoch/train.ipynb#ch0000014?line=16'>17</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\hk3pz\\Code\\fandecaoch\\.env\\lib\\site-packages\\torch\\nn\\modules\\module.py:907\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=902'>903</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=903'>904</a>\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=904'>905</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m--> <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=906'>907</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[1;32mc:\\Users\\hk3pz\\Code\\fandecaoch\\.env\\lib\\site-packages\\torch\\nn\\modules\\module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=575'>576</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=576'>577</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=577'>578</a>\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=579'>580</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=580'>581</a>\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=581'>582</a>\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=582'>583</a>\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=587'>588</a>\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=588'>589</a>\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hk3pz\\Code\\fandecaoch\\.env\\lib\\site-packages\\torch\\nn\\modules\\module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=575'>576</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=576'>577</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=577'>578</a>\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=579'>580</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=580'>581</a>\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=581'>582</a>\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=582'>583</a>\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=587'>588</a>\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=588'>589</a>\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hk3pz\\Code\\fandecaoch\\.env\\lib\\site-packages\\torch\\nn\\modules\\module.py:601\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=596'>597</a>\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=597'>598</a>\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=598'>599</a>\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=599'>600</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=600'>601</a>\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=601'>602</a>\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=602'>603</a>\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32mc:\\Users\\hk3pz\\Code\\fandecaoch\\.env\\lib\\site-packages\\torch\\nn\\modules\\module.py:905\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=901'>902</a>\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=902'>903</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=903'>904</a>\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m--> <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=904'>905</a>\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 4.00 GiB total capacity; 3.09 GiB already allocated; 0 bytes free; 3.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=configuration)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr = learning_rate,\n",
    "    eps = epsilon\n",
    ")\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps = warmup_steps, \n",
    "    num_training_steps = total_steps\n",
    ")\n",
    "\n",
    "def format_time(elapsed):\n",
    "    return str(datetime.timedelta(seconds=int(round((elapsed)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 5 ========\n",
      "Training...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 14.00 MiB (GPU 0; 4.00 GiB total capacity; 3.09 GiB already allocated; 0 bytes free; 3.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hk3pz\\Code\\fandecaoch\\train.ipynb Cell 16'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hk3pz/Code/fandecaoch/train.ipynb#ch0000018?line=24'>25</a>\u001b[0m b_masks \u001b[39m=\u001b[39m batch[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hk3pz/Code/fandecaoch/train.ipynb#ch0000018?line=26'>27</a>\u001b[0m model\u001b[39m.\u001b[39mzero_grad()        \n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hk3pz/Code/fandecaoch/train.ipynb#ch0000018?line=28'>29</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hk3pz/Code/fandecaoch/train.ipynb#ch0000018?line=29'>30</a>\u001b[0m     b_input_ids,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hk3pz/Code/fandecaoch/train.ipynb#ch0000018?line=30'>31</a>\u001b[0m     labels\u001b[39m=\u001b[39;49mb_labels, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hk3pz/Code/fandecaoch/train.ipynb#ch0000018?line=31'>32</a>\u001b[0m     attention_mask \u001b[39m=\u001b[39;49m b_masks,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hk3pz/Code/fandecaoch/train.ipynb#ch0000018?line=32'>33</a>\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hk3pz/Code/fandecaoch/train.ipynb#ch0000018?line=33'>34</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hk3pz/Code/fandecaoch/train.ipynb#ch0000018?line=35'>36</a>\u001b[0m loss \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]  \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hk3pz/Code/fandecaoch/train.ipynb#ch0000018?line=37'>38</a>\u001b[0m batch_loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\hk3pz\\Code\\fandecaoch\\.env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\hk3pz\\Code\\fandecaoch\\.env\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1046\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1037'>1038</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1038'>1039</a>\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1039'>1040</a>\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1040'>1041</a>\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1041'>1042</a>\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1042'>1043</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1043'>1044</a>\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1045'>1046</a>\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[0;32m   <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1046'>1047</a>\u001b[0m     input_ids,\n\u001b[0;32m   <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1047'>1048</a>\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1048'>1049</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1049'>1050</a>\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1050'>1051</a>\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1051'>1052</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1052'>1053</a>\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1053'>1054</a>\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1054'>1055</a>\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[0;32m   <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1055'>1056</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1056'>1057</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1057'>1058</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1058'>1059</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1059'>1060</a>\u001b[0m )\n\u001b[0;32m   <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1060'>1061</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1062'>1063</a>\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hk3pz\\Code\\fandecaoch\\.env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\hk3pz\\Code\\fandecaoch\\.env\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:889\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=878'>879</a>\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=879'>880</a>\u001b[0m         create_custom_forward(block),\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=880'>881</a>\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=885'>886</a>\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=886'>887</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=887'>888</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=888'>889</a>\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=889'>890</a>\u001b[0m         hidden_states,\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=890'>891</a>\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=891'>892</a>\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=892'>893</a>\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=893'>894</a>\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=894'>895</a>\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=895'>896</a>\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=896'>897</a>\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=897'>898</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=899'>900</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=900'>901</a>\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\hk3pz\\Code\\fandecaoch\\.env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\hk3pz\\Code\\fandecaoch\\.env\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:390\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=387'>388</a>\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=388'>389</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1(hidden_states)\n\u001b[1;32m--> <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=389'>390</a>\u001b[0m attn_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=390'>391</a>\u001b[0m     hidden_states,\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=391'>392</a>\u001b[0m     layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=392'>393</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=393'>394</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=394'>395</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=395'>396</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=396'>397</a>\u001b[0m )\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=397'>398</a>\u001b[0m attn_output \u001b[39m=\u001b[39m attn_outputs[\u001b[39m0\u001b[39m]  \u001b[39m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=398'>399</a>\u001b[0m outputs \u001b[39m=\u001b[39m attn_outputs[\u001b[39m1\u001b[39m:]\n",
      "File \u001b[1;32mc:\\Users\\hk3pz\\Code\\fandecaoch\\.env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\hk3pz\\Code\\fandecaoch\\.env\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:312\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=309'>310</a>\u001b[0m     attention_mask \u001b[39m=\u001b[39m encoder_attention_mask\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=310'>311</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=311'>312</a>\u001b[0m     query, key, value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mc_attn(hidden_states)\u001b[39m.\u001b[39msplit(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit_size, dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=313'>314</a>\u001b[0m query \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_split_heads(query, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=314'>315</a>\u001b[0m key \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_split_heads(key, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\n",
      "File \u001b[1;32mc:\\Users\\hk3pz\\Code\\fandecaoch\\.env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\hk3pz\\Code\\fandecaoch\\.env\\lib\\site-packages\\transformers\\pytorch_utils.py:107\u001b[0m, in \u001b[0;36mConv1D.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/pytorch_utils.py?line=104'>105</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/pytorch_utils.py?line=105'>106</a>\u001b[0m     size_out \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnf,)\n\u001b[1;32m--> <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/pytorch_utils.py?line=106'>107</a>\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49maddmm(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, x\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, x\u001b[39m.\u001b[39;49msize(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight)\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/pytorch_utils.py?line=107'>108</a>\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(size_out)\n\u001b[0;32m    <a href='file:///c%3A/Users/hk3pz/Code/fandecaoch/.env/lib/site-packages/transformers/pytorch_utils.py?line=108'>109</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 0; 4.00 GiB total capacity; 3.09 GiB already allocated; 0 bytes free; 3.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "total_t0 = time.time()\n",
    "training_stats = []\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    total_train_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_labels = batch[0].to(device)\n",
    "        b_masks = batch[1].to(device)\n",
    "\n",
    "        model.zero_grad()        \n",
    "\n",
    "        outputs = model(\n",
    "            b_input_ids,\n",
    "            labels=b_labels, \n",
    "            attention_mask = b_masks,\n",
    "            token_type_ids=None\n",
    "        )\n",
    "\n",
    "        loss = outputs[0]  \n",
    "\n",
    "        batch_loss = loss.item()\n",
    "        total_train_loss += batch_loss\n",
    "\n",
    "        # Get sample every x batches.\n",
    "        if step % sample_every == 0 and not step == 0:\n",
    "\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}. Loss: {:>5,}.   Elapsed: {:}.'.format(step, len(train_dataloader), batch_loss, elapsed))\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            sample_outputs = model.generate(\n",
    "                bos_token_id=random.randint(1,30000),\n",
    "                do_sample=True,   \n",
    "                top_k=50, \n",
    "                max_length = 200,\n",
    "                top_p=0.95, \n",
    "                num_return_sequences=1\n",
    "            )\n",
    "            for i, sample_output in enumerate(sample_outputs):\n",
    "                print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n",
    "            \n",
    "            model.train()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)       \n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    model.eval()\n",
    "\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_labels = batch[0].to(device)\n",
    "        b_masks = batch[1].to(device)\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "            outputs = model(\n",
    "                b_input_ids, \n",
    "                attention_mask = b_masks,\n",
    "                labels=b_labels\n",
    "            )\n",
    "            loss = outputs[0]  \n",
    "            \n",
    "        batch_loss = loss.item()\n",
    "        total_eval_loss += batch_loss        \n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    validation_time = format_time(time.time() - t0)    \n",
    "\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c2f6e16b18969bc081152769eb6236619ccaebf458042e3332e2a867026bbb26"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
